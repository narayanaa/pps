#!/usr/bin/env python3
"""
Quick Sanity Check Before Git Check-in
"""

import os
from pathlib import Path

def quick_sanity_check():
    print("ğŸ” FINAL SANITY CHECK - WEB SCRAPER PROJECT")
    print("="*60)
    
    # Check current directory structure
    python_dir = Path(".")
    
    print("\nğŸ“ DIRECTORY STRUCTURE:")
    print("-" * 30)
    essential_dirs = ['scrapers', 'parsers', 'doc', 'tests']
    for dirname in essential_dirs:
        if (python_dir / dirname).exists():
            print(f"   âœ… {dirname}/")
        else:
            print(f"   âŒ {dirname}/ - MISSING")
    
    print("\nğŸ“„ ESSENTIAL FILES:")
    print("-" * 30)
    essential_files = [
        'README.md',
        'requirements.txt', 
        'config.json',
        'run_scraper.py'
    ]
    
    for filename in essential_files:
        if (python_dir / filename).exists():
            size = (python_dir / filename).stat().st_size
            print(f"   âœ… {filename} ({size//1024}KB)")
        else:
            print(f"   âŒ {filename} - MISSING")
    
    print("\nğŸ§¹ CLEANUP VERIFICATION:")
    print("-" * 30)
    
    # Check for cleanup opportunities
    cleanup_items = []
    
    # Large cache files
    cache_dir = python_dir / '.cache'
    if cache_dir.exists():
        cache_files = list(cache_dir.glob('*.cache'))
        if cache_files:
            cache_size = sum(f.stat().st_size for f in cache_files)
            cleanup_items.append(f"ğŸ“¦ .cache/ directory ({len(cache_files)} files, {cache_size//1024//1024}MB)")
    
    # Python cache
    pycache_dirs = list(python_dir.rglob('__pycache__'))
    if pycache_dirs:
        cleanup_items.append(f"ğŸ {len(pycache_dirs)} __pycache__ directories")
    
    # Log files
    log_files = list(python_dir.rglob('*.log'))
    if log_files:
        log_size = sum(f.stat().st_size for f in log_files)
        cleanup_items.append(f"ğŸ“ {len(log_files)} log files ({log_size//1024}KB)")
    
    # Test output files
    test_outputs = list(python_dir.rglob('*test_output*.json'))
    if test_outputs:
        cleanup_items.append(f"ğŸ§ª {len(test_outputs)} test output files")
    
    if cleanup_items:
        print("   âš ï¸  CLEANUP RECOMMENDATIONS:")
        for item in cleanup_items:
            print(f"      {item}")
    else:
        print("   âœ… Project appears clean")
    
    print("\nğŸš€ PROJECT STATUS:")
    print("-" * 30)
    
    # Check if analysis files were removed
    if not (python_dir / "analyze_project_size.py").exists():
        print("   âœ… Analysis files removed")
    else:
        print("   âš ï¸  Analysis files still present")
    
    # Core functionality check
    try:
        from scrapers.psense.web.scraper import WebScraper
        print("   âœ… Main scraper imports successfully")
    except ImportError as e:
        print(f"   âŒ Scraper import failed: {e}")
    
    try:
        from parsers import psense
        print("   âœ… Parsers import successfully") 
    except ImportError as e:
        print(f"   âŒ Parsers import failed: {e}")
    
    print("\nğŸ’¡ READY FOR CHECK-IN:")
    print("-" * 30)
    print("   1. âœ… Core files present")
    print("   2. âœ… Analysis files cleaned up")
    print("   3. ğŸ“¦ Consider cleaning cache before zip")
    print("   4. ğŸ”’ All imports working")
    
    print("\nğŸ¯ FINAL CLEANUP COMMAND (optional):")
    print("   rm -rf .cache __pycache__ .pytest_cache")
    print("   find . -name '*.log' -size +100k -delete")

if __name__ == "__main__":
    quick_sanity_check()